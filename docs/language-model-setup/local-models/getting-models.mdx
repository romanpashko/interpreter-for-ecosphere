---
title: Getting Models
---

You can run any [`GGUF`](https://huggingface.co/models?search=gguf) model hosted on Hugging Face with Open Interpreter:

<CodeGroup>

```bash Terminal
interpreter --local --model huggingface/<user>/<model>
```

```python Python
import interpreter

interpreter.local = True
interpreter.model = "huggingface/<user>/<model>"
interpreter.chat()
```

</CodeGroup>

The command above will let you choose your quantization settings, download the model, then run it.

### Example

---

For example, if the repo URL was:

```
https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
```

The command to download and run this language model in Open Interpreter would be:

<CodeGroup>

```bash Terminal
interpreter --local --model huggingface/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
```

```python Python
import interpreter

interpreter.local = True
interpreter.model = "huggingface/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
interpreter.chat()
```

</CodeGroup>

<br/>

<Info>Run this command with your internet on once to download the model.</Info>

### Get Models

---

<Card
  title="GGUF models on HuggingFace"
  icon="arrow-up-right"
  iconType="solid"
  href="https://huggingface.co/models?search=gguf"
>
</Card>
<br/>
Open Interpreter can run any of the models at the link above.

Click one, then copy the text after the second-to-last `/` to use in the command above.